{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6640ad36",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c0f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import sys\n",
    "import pyreadr\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "sys.path.append(\"../snpe/\")\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "#print(sys.path)\n",
    "from snpe.simulations import simulator_class, marketplace_simulator_class\n",
    "\n",
    "from snpe.utils.functions import nn_converged # Function to authomatically stop convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b8e96",
   "metadata": {},
   "source": [
    "### A. Load the data: using the real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76fe49",
   "metadata": {},
   "source": [
    "Load reviews real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051ef935",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pyreadr.read_r(\"data/reviews_bazaarvoice_main_vars.Rds\")\n",
    "reviews = reviews[None] #extracting dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc33ee",
   "metadata": {},
   "source": [
    "Load embeddings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602012ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embbeddings = pd.read_csv(\"data/productspace.tsv\", sep = \"\\t\", header=None)\n",
    "minus_prod = lambda s : s.replace(\"product_\", \"\")\n",
    "embbeddings[0] = embbeddings[0].apply(func= minus_prod)\n",
    "embbeddings = embbeddings.set_index(0, drop=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5fb9c",
   "metadata": {},
   "source": [
    "### B. Data pre-processing, Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3dcfed",
   "metadata": {},
   "source": [
    "Assigning each \"asin\" to its corresponding embbedding in a dictionary for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143020af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews[\"asin\"].unique()) # reviews of 1406 real products\n",
    "len(embbeddings[0].unique()) # embeddings for 31704 real products\n",
    "\n",
    "coincidences = []\n",
    "embd_dict = {}\n",
    "\n",
    "for i in reviews[\"asin\"].unique():\n",
    "    if str(i) in embbeddings[0].unique():\n",
    "        embd_dict[i] = np.array(embbeddings.loc[str(i)][1:])\n",
    "\n",
    "#embd_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f5788",
   "metadata": {},
   "source": [
    "Separating data by product, storing it in different dictionaries for train, validation and test (30%, 20%, 50%), all of this while preserving the appropriate chronological order of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bffaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dict_train = {}\n",
    "frames_dict_validation = {}\n",
    "frames_dict_test = {}\n",
    "\n",
    "for i in embd_dict.keys():\n",
    "    frames_dict_train[i] = reviews[reviews[\"asin\"]== i].sort_values(by = \"ReviewId\").iloc[:int(reviews[reviews[\"asin\"]== i].shape[0]//3.3333)].reset_index()\n",
    "    frames_dict_validation[i] = reviews[reviews[\"asin\"]== i].sort_values(by = \"ReviewId\").iloc[int(reviews[reviews[\"asin\"]== i].shape[0]//3.3333):reviews[reviews[\"asin\"]== i].shape[0]//(2)].reset_index()\n",
    "    frames_dict_test[i] = reviews[reviews[\"asin\"]== i].sort_values(by = \"ReviewId\").iloc[reviews[reviews[\"asin\"]== i].shape[0]//(2):].reset_index()\n",
    "    \n",
    "    \n",
    "for i in embd_dict.keys():\n",
    "    frames_dict_train[i]['emb'] = [embd_dict.get(i) for e in range(frames_dict_train[i].shape[0])]\n",
    "    frames_dict_validation[i]['emb'] = [embd_dict.get(i) for e in range(frames_dict_validation[i].shape[0])]\n",
    "    frames_dict_test[i]['emb'] = [embd_dict.get(i) for e in range(frames_dict_test[i].shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02946de9",
   "metadata": {},
   "source": [
    "Sample of how data is split for product \"1017360\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc3fc3",
   "metadata": {},
   "source": [
    "Train (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a86d9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>rootcategoryid</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>9186319</td>\n",
       "      <td>2011-02-16</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>9312754</td>\n",
       "      <td>2011-03-02</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>9313265</td>\n",
       "      <td>2011-03-02</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>9316925</td>\n",
       "      <td>2011-03-03</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>9320306</td>\n",
       "      <td>2011-03-03</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>164</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>13467012</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>165</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>13835766</td>\n",
       "      <td>2012-02-15</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>166</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>14860492</td>\n",
       "      <td>2012-03-07</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>167</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>14940874</td>\n",
       "      <td>2012-03-08</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>168</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>15862596</td>\n",
       "      <td>2012-03-14</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     asin  overall  ReviewId unixReviewTime  rootcategoryid  \\\n",
       "0     104  1017360        4   9186319     2011-02-16        33006169   \n",
       "1     106  1017360        5   9312754     2011-03-02        33006169   \n",
       "2     105  1017360        5   9313265     2011-03-02        33006169   \n",
       "3     108  1017360        5   9316925     2011-03-03        33006169   \n",
       "4     107  1017360        5   9320306     2011-03-03        33006169   \n",
       "..    ...      ...      ...       ...            ...             ...   \n",
       "61    164  1017360        5  13467012     2012-02-05        33006169   \n",
       "62    165  1017360        5  13835766     2012-02-15        33006169   \n",
       "63    166  1017360        5  14860492     2012-03-07        33006169   \n",
       "64    167  1017360        1  14940874     2012-03-08        33006169   \n",
       "65    168  1017360        5  15862596     2012-03-14        33006169   \n",
       "\n",
       "                                                  emb  \n",
       "0   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "1   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "2   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "3   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "4   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "..                                                ...  \n",
       "61  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "62  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "63  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "64  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "65  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "\n",
       "[66 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_dict_train.get(1017360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd077864",
   "metadata": {},
   "source": [
    "Validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8fbaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>rootcategoryid</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>16003890</td>\n",
       "      <td>2012-03-21</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>16004163</td>\n",
       "      <td>2012-03-21</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>16009516</td>\n",
       "      <td>2012-03-21</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172</td>\n",
       "      <td>1017360</td>\n",
       "      <td>3</td>\n",
       "      <td>16133666</td>\n",
       "      <td>2012-03-28</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>16495266</td>\n",
       "      <td>2012-04-18</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>174</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>16601854</td>\n",
       "      <td>2012-04-26</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>17131022</td>\n",
       "      <td>2012-05-10</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>176</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>17710339</td>\n",
       "      <td>2012-05-23</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>19398032</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>178</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>19723381</td>\n",
       "      <td>2012-06-19</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>180</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>19729723</td>\n",
       "      <td>2012-06-20</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>179</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>19732931</td>\n",
       "      <td>2012-06-20</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>181</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>20033821</td>\n",
       "      <td>2012-07-11</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>182</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>20467322</td>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>183</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>20639603</td>\n",
       "      <td>2012-08-17</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>184</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>21418130</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>185</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>22348076</td>\n",
       "      <td>2012-10-24</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>186</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>22461164</td>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>318</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>22492596</td>\n",
       "      <td>None</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>187</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>22494802</td>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>188</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>22576480</td>\n",
       "      <td>2012-11-06</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>189</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>22639157</td>\n",
       "      <td>2012-11-07</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>190</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>22747601</td>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>191</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>22775354</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>192</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>24136178</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>193</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>24529447</td>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>194</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>24717231</td>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>195</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>24725356</td>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>196</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>24742831</td>\n",
       "      <td>2013-02-15</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>197</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>25064762</td>\n",
       "      <td>2013-02-28</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>198</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>25258578</td>\n",
       "      <td>2013-03-11</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>199</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>25565047</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>200</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>25780291</td>\n",
       "      <td>2013-04-03</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>203</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26056307</td>\n",
       "      <td>2013-04-10</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>202</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26061571</td>\n",
       "      <td>2013-04-10</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>201</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>26063016</td>\n",
       "      <td>2013-04-10</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>204</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26222749</td>\n",
       "      <td>2013-04-18</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>206</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26355611</td>\n",
       "      <td>2013-04-24</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>205</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26370776</td>\n",
       "      <td>2013-04-24</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>207</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26384552</td>\n",
       "      <td>2013-04-25</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>208</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26390010</td>\n",
       "      <td>2013-04-26</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>209</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26413660</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>210</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26462259</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>212</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>26919601</td>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>211</td>\n",
       "      <td>1017360</td>\n",
       "      <td>2</td>\n",
       "      <td>26919727</td>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     asin  overall  ReviewId unixReviewTime  rootcategoryid  \\\n",
       "0     171  1017360        5  16003890     2012-03-21        33006169   \n",
       "1     169  1017360        1  16004163     2012-03-21        33006169   \n",
       "2     170  1017360        4  16009516     2012-03-21        33006169   \n",
       "3     172  1017360        3  16133666     2012-03-28        33006169   \n",
       "4     173  1017360        5  16495266     2012-04-18        33006169   \n",
       "5     174  1017360        5  16601854     2012-04-26        33006169   \n",
       "6     175  1017360        5  17131022     2012-05-10        33006169   \n",
       "7     176  1017360        4  17710339     2012-05-23        33006169   \n",
       "8     177  1017360        1  19398032     2012-06-12        33006169   \n",
       "9     178  1017360        4  19723381     2012-06-19        33006169   \n",
       "10    180  1017360        5  19729723     2012-06-20        33006169   \n",
       "11    179  1017360        5  19732931     2012-06-20        33006169   \n",
       "12    181  1017360        1  20033821     2012-07-11        33006169   \n",
       "13    182  1017360        4  20467322     2012-08-10        33006169   \n",
       "14    183  1017360        5  20639603     2012-08-17        33006169   \n",
       "15    184  1017360        5  21418130     2012-09-20        33006169   \n",
       "16    185  1017360        1  22348076     2012-10-24        33006169   \n",
       "17    186  1017360        4  22461164     2012-10-31        33006169   \n",
       "18    318  1017360        5  22492596           None        33006169   \n",
       "19    187  1017360        5  22494802     2012-11-01        33006169   \n",
       "20    188  1017360        5  22576480     2012-11-06        33006169   \n",
       "21    189  1017360        5  22639157     2012-11-07        33006169   \n",
       "22    190  1017360        1  22747601     2012-11-14        33006169   \n",
       "23    191  1017360        4  22775354     2012-11-16        33006169   \n",
       "24    192  1017360        5  24136178     2013-01-18        33006169   \n",
       "25    193  1017360        5  24529447     2013-02-07        33006169   \n",
       "26    194  1017360        4  24717231     2013-02-13        33006169   \n",
       "27    195  1017360        5  24725356     2013-02-14        33006169   \n",
       "28    196  1017360        5  24742831     2013-02-15        33006169   \n",
       "29    197  1017360        4  25064762     2013-02-28        33006169   \n",
       "30    198  1017360        5  25258578     2013-03-11        33006169   \n",
       "31    199  1017360        5  25565047     2013-03-28        33006169   \n",
       "32    200  1017360        5  25780291     2013-04-03        33006169   \n",
       "33    203  1017360        5  26056307     2013-04-10        33006169   \n",
       "34    202  1017360        5  26061571     2013-04-10        33006169   \n",
       "35    201  1017360        4  26063016     2013-04-10        33006169   \n",
       "36    204  1017360        5  26222749     2013-04-18        33006169   \n",
       "37    206  1017360        5  26355611     2013-04-24        33006169   \n",
       "38    205  1017360        5  26370776     2013-04-24        33006169   \n",
       "39    207  1017360        5  26384552     2013-04-25        33006169   \n",
       "40    208  1017360        5  26390010     2013-04-26        33006169   \n",
       "41    209  1017360        5  26413660     2013-04-28        33006169   \n",
       "42    210  1017360        5  26462259     2013-05-01        33006169   \n",
       "43    212  1017360        5  26919601     2013-05-14        33006169   \n",
       "44    211  1017360        2  26919727     2013-05-14        33006169   \n",
       "\n",
       "                                                  emb  \n",
       "0   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "1   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "2   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "3   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "4   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "5   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "6   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "7   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "8   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "9   [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "10  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "11  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "12  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "13  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "14  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "15  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "16  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "17  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "18  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "19  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "20  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "21  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "22  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "23  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "24  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "25  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "26  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "27  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "28  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "29  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "30  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "31  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "32  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "33  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "34  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "35  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "36  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "37  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "38  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "39  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "40  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "41  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "42  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "43  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "44  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_dict_validation.get(1017360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395a0a8",
   "metadata": {},
   "source": [
    "Test (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071c6bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>rootcategoryid</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>27280987</td>\n",
       "      <td>2013-05-31</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214</td>\n",
       "      <td>1017360</td>\n",
       "      <td>1</td>\n",
       "      <td>27293847</td>\n",
       "      <td>2013-06-03</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>27397350</td>\n",
       "      <td>2013-06-06</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>27505296</td>\n",
       "      <td>2013-06-12</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>217</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>27754074</td>\n",
       "      <td>2013-06-26</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>309</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>46453667</td>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>322</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>46640149</td>\n",
       "      <td>None</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>310</td>\n",
       "      <td>1017360</td>\n",
       "      <td>4</td>\n",
       "      <td>46646413</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>311</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>46827663</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>312</td>\n",
       "      <td>1017360</td>\n",
       "      <td>5</td>\n",
       "      <td>47064414</td>\n",
       "      <td>None</td>\n",
       "      <td>33006169</td>\n",
       "      <td>[0.0029107, 0.0199935, -0.0198807, 0.00917163,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     asin  overall  ReviewId unixReviewTime  rootcategoryid  \\\n",
       "0      213  1017360        5  27280987     2013-05-31        33006169   \n",
       "1      214  1017360        1  27293847     2013-06-03        33006169   \n",
       "2      215  1017360        5  27397350     2013-06-06        33006169   \n",
       "3      216  1017360        5  27505296     2013-06-12        33006169   \n",
       "4      217  1017360        5  27754074     2013-06-26        33006169   \n",
       "..     ...      ...      ...       ...            ...             ...   \n",
       "106    309  1017360        5  46453667     2015-03-04        33006169   \n",
       "107    322  1017360        5  46640149           None        33006169   \n",
       "108    310  1017360        4  46646413     2015-03-10        33006169   \n",
       "109    311  1017360        5  46827663     2015-03-17        33006169   \n",
       "110    312  1017360        5  47064414           None        33006169   \n",
       "\n",
       "                                                   emb  \n",
       "0    [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "1    [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "2    [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "3    [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "4    [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "..                                                 ...  \n",
       "106  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "107  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "108  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "109  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "110  [0.0029107, 0.0199935, -0.0198807, 0.00917163,...  \n",
       "\n",
       "[111 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_dict_test.get(1017360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e42f1a",
   "metadata": {},
   "source": [
    "Detecting and discarding products which do not have enough reviews to satisfy the \"size\" requirements of the sliding window format. In this case sliding window length/width is 10 reviews. (i.e. a review in t+1 is predicted on the basis of the reviews from t-9 to t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ec12b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699 out of 1404 have enough reviews to be included in the\n",
      "final data set given the specified window length of 10. The remainder where discarded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_length = 10\n",
    "\n",
    "list_bool_length = []\n",
    "keys_to_discard = []\n",
    "\n",
    "\n",
    "for k in frames_dict_validation.keys():\n",
    "    list_bool_length.append(frames_dict_validation.get(k).shape[0] >= (window_length + 1))\n",
    "    if frames_dict_validation.get(k).shape[0] < (window_length + 1):\n",
    "        keys_to_discard.append(k)\n",
    "\n",
    "for k in keys_to_discard:\n",
    "    del frames_dict_train[k]\n",
    "    del frames_dict_validation[k]\n",
    "    del frames_dict_test[k]\n",
    "    \n",
    "# 699 out of 1404 products have enough reviews to build at least 1 validation sample\n",
    "print(\n",
    "    f'''{str(sum(list_bool_length))} out of {str(len(list_bool_length))} have enough reviews to be included in the\n",
    "final data set given the specified window length of {window_length}. The remainder where discarded.\n",
    "''' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b03c64",
   "metadata": {},
   "source": [
    "Transformation of each row of each product the data frames created above into 1-D tensors containing each rating and embedding. This particular configuration of each tensor including rating+embedding in its lower dimension has been designed this way to make it easier to feed the data to the model while complying with the requirements of pytorch's dataset and dataloader classes that wrap the data so it can be provided to the model. These tensors are then stacked to form a tensor of (n, 101) dimensions where n is the corresponding number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60a9950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionaries to store each product's tensors\n",
    "tens_train = {}\n",
    "tens_validation = {}\n",
    "tens_test = {}\n",
    "\n",
    "# Tensors for each row are created and stores in a list for the train, validation and test sets\n",
    "for p in frames_dict_train.keys():\n",
    "    #print(p)\n",
    "    a_product_list = []\n",
    "    for i in range(frames_dict_train.get(p).shape[0]):\n",
    "        temp_list = []\n",
    "        temp_list.append(frames_dict_train.get(p)['overall'][i])\n",
    "        for k in frames_dict_train.get(p)['emb'][i]:\n",
    "            temp_list.append(k)\n",
    "        a_product_list.append(torch.FloatTensor(temp_list))\n",
    "    tens_train[p] = a_product_list\n",
    "    #print(a_product_list)\n",
    "    \n",
    "for p in frames_dict_train.keys():\n",
    "    #print(p)\n",
    "    a_product_list = []\n",
    "    for i in range(frames_dict_validation.get(p).shape[0]):\n",
    "        temp_list = []\n",
    "        temp_list.append(frames_dict_validation.get(p)['overall'][i])\n",
    "        for k in frames_dict_validation.get(p)['emb'][i]:\n",
    "            temp_list.append(k)\n",
    "        a_product_list.append(torch.FloatTensor(temp_list))\n",
    "    tens_validation[p] = a_product_list\n",
    "    #print(a_product_list)\n",
    "    \n",
    "for p in frames_dict_train.keys():\n",
    "    #print(p)\n",
    "    a_product_list = []\n",
    "    for i in range(frames_dict_test.get(p).shape[0]):\n",
    "        temp_list = []\n",
    "        temp_list.append(frames_dict_test.get(p)['overall'][i])\n",
    "        for k in frames_dict_test.get(p)['emb'][i]:\n",
    "            temp_list.append(k)\n",
    "        a_product_list.append(torch.FloatTensor(temp_list))\n",
    "    tens_test[p] = a_product_list\n",
    "    #print(a_product_list)\n",
    "\n",
    "# Stacking all tensors in all dictionaries to have a tensor of (n, 101) per product intead of a list of tensors    \n",
    "for t in tens_train.keys():\n",
    "    tens_train[t] = torch.stack([i for i in tens_train.get(t)])\n",
    "    tens_validation[t] = torch.stack([i for i in tens_validation.get(t)])\n",
    "    tens_test[t] = torch.stack([i for i in tens_test.get(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf9da5",
   "metadata": {},
   "source": [
    "Now we have 3 dictionaries containing the train, validation and test sets for each product. For each product, and set we have a tensor of shape (n_i, 101) as mentioned above. For example, if a given product has N ratings, in the dictionary containing the training sets the value associated to its product ID key we will have a tensor of the shape (N\\*0.3, 101), in the validation dictionary a tensor with a shape of (N\\*0.2, 101) and finally in the test dictionary a tensor of the shape (N\\*0.5, 101). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77918365",
   "metadata": {},
   "source": [
    "As you can see in the example below, the first element of the tensor is the corresponding rating for a given T, followed by the 100-d embedding of the product to which it belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d849703",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0000e+00,  2.9107e-03,  1.9994e-02, -1.9881e-02,  9.1716e-03,\n",
       "         1.9235e-02, -7.0281e-03, -5.3703e-03,  9.5135e-04, -2.2286e-02,\n",
       "         1.3778e-02, -4.1408e-02, -1.5520e-02, -2.1755e-03,  2.2203e-02,\n",
       "        -2.8938e-02, -1.2219e-02,  2.6656e-02, -1.3024e-02,  4.4244e-02,\n",
       "        -4.0905e-03, -7.8043e-03,  3.3163e-02, -1.1404e-02,  1.7175e-03,\n",
       "         1.5078e-02,  1.8824e-02, -9.7987e-03, -1.1135e-02, -1.2177e-02,\n",
       "         3.0665e-02,  2.0524e-02, -1.3165e-02,  3.9614e-02,  1.6273e-03,\n",
       "         7.5149e-03, -6.4778e-03,  1.4776e-02, -1.4481e-03,  1.7568e-02,\n",
       "        -8.0347e-03,  1.2923e-03,  2.9955e-02,  2.9933e-02, -4.4433e-03,\n",
       "        -8.8186e-03,  1.1811e-03,  5.1223e-02,  1.6176e-02,  2.6995e-02,\n",
       "         4.1468e-03,  4.4859e-02, -2.3765e-02,  2.2506e-02,  2.1003e-02,\n",
       "        -2.8345e-02, -1.5568e-02,  2.1291e-02, -2.0661e-02, -3.0756e-03,\n",
       "        -4.4781e-03, -2.6516e-02, -4.5507e-03, -2.0287e-02,  2.4336e-02,\n",
       "         2.0215e-02,  3.6595e-03,  4.6210e-03,  2.1822e-02, -1.4586e-02,\n",
       "        -2.4191e-02, -1.9461e-02, -5.4608e-03,  2.7833e-03,  2.1811e-02,\n",
       "        -2.0832e-02,  2.8958e-02, -1.4556e-02,  2.8898e-02, -1.2929e-02,\n",
       "        -1.5511e-02, -1.5661e-04,  3.8739e-03,  8.0448e-03, -2.0817e-04,\n",
       "        -7.5605e-03, -2.0783e-02,  2.5677e-02,  3.4510e-02, -4.0680e-03,\n",
       "         3.7819e-02, -1.2440e-02, -1.5970e-02, -7.2217e-03, -7.1946e-03,\n",
       "        -2.0381e-02, -3.6273e-03, -2.2518e-03,  9.7876e-03, -2.2670e-02,\n",
       "         5.7586e-03])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens_train.get(1017360)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb1fc5",
   "metadata": {},
   "source": [
    "A bit of context on the cells below:\n",
    "In pytorch you can either import pre existing datasets or use your own. To use your own you have to define it as aninstance of the dataset class, including three functions: `__init__, __len__` and `__getitem__` .\n",
    "\n",
    "- Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "- More about this can be found in pytorch's documentation: \n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "- Main source for the desgin of the `SlidingDataset` class: https://discuss.pytorch.org/t/dataloader-for-a-lstm-model-with-a-sliding-window/22235 . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b665061",
   "metadata": {},
   "source": [
    "Defining an appropriate dataset class according to the needs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cffd7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingDataset(Dataset):\n",
    "    '''\n",
    "    This SlidingDataset class is conformed so each input is comprised of the last N observations (from t-n to t) \n",
    "    of the series, where n  is the size of the sliding window, and the target is the corresponding observation in \n",
    "    t+1.\n",
    "    '''\n",
    "    def __init__(self, data, window):\n",
    "        self.data = data\n",
    "        #self.data = torch.from_numpy(data)\n",
    "        self.window = window\n",
    "        \n",
    "    def __len__(self): \n",
    "        '''\n",
    "        The __len__ function returns the number of samples in our dataset. In this case intuitively we have\n",
    "        a number of samples equal to the total number of elements in the time series minus the size of the \n",
    "        sliding window. (i.e. How many previous observations conform each input)\n",
    "        '''\n",
    "        return len(self.data)-self.window\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        \n",
    "        ratings = torch.from_numpy(np.array([float(self.data[i][0]) for i in range(len(self.data))]))\n",
    "        \n",
    "        return  (ratings[index : index+self.window].double(), self.data[0][1:]) , ratings[index+self.window].double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21ea22",
   "metadata": {},
   "source": [
    "Now, with the sliding dataset class defined the data can be processed by it and then subsequently to the dataloader class. This is done for all products across the three previously defined dictionaries for training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e2c855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell execution completed\n"
     ]
    }
   ],
   "source": [
    "datasets_list_train = []\n",
    "datasets_list_validation = []\n",
    "datasets_list_test = []\n",
    "sliding_window_size = 10\n",
    "\n",
    "for i in frames_dict_train.keys():\n",
    "    a = SlidingDataset(torch.from_numpy(np.array(tens_train.get(i))), sliding_window_size)\n",
    "    b = SlidingDataset(torch.from_numpy(np.array(tens_validation.get(i))), sliding_window_size)\n",
    "    c = SlidingDataset(torch.from_numpy(np.array(tens_test.get(i))), sliding_window_size)\n",
    "    datasets_list_train.append(a)\n",
    "    datasets_list_validation.append(b)\n",
    "    datasets_list_test.append(c)\n",
    "\n",
    "final_train_set= torch.utils.data.ConcatDataset(datasets_list_train)\n",
    "final_validation_set= torch.utils.data.ConcatDataset(datasets_list_validation)\n",
    "final_test_set= torch.utils.data.ConcatDataset(datasets_list_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(final_train_set,batch_size, shuffle = True) \n",
    "validation_dataloader = DataLoader(final_validation_set,batch_size) #, shuffle = True)\n",
    "test_dataloader = DataLoader(final_test_set,batch_size) #, shuffle = True)  \n",
    "\n",
    "print(\"Cell execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbcdc2",
   "metadata": {},
   "source": [
    "Example of how the elements of the resulting dataloader objects are structured.\n",
    "\n",
    "For each batch there are 32 feature-target pairs. The features are subdivided in two, ratings and embeddings. Each feature element contains: \n",
    "\n",
    "   - 1. 32 rating sliding window inputs and \n",
    "   - 2. Their correspondent 32 embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef58a9",
   "metadata": {},
   "source": [
    "Below there is an example how input-target pairs are configured by the dataloader class in this case. The input is composed of two \"parts\", an array of 10 ratings (corresponding to the last 10 ratings before the target rating occurred, going from T-9 to T0), and the 100-d embedding of the product. As mentioned above, the target is the rating at T+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23b5f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      " -> Sliding window sample\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.], dtype=torch.float64)\n",
      "#####################\n",
      " -> Embbedding\n",
      "tensor([ 0.0176, -0.0038, -0.0071,  0.0003,  0.0019, -0.0145,  0.0096,  0.0048,\n",
      "        -0.0047, -0.0014, -0.0003, -0.0237, -0.0081, -0.0096,  0.0094, -0.0008,\n",
      "        -0.0124, -0.0025,  0.0033, -0.0073, -0.0016,  0.0079,  0.0120,  0.0083,\n",
      "         0.0084,  0.0005,  0.0095,  0.0121,  0.0002,  0.0037,  0.0036,  0.0096,\n",
      "         0.0033,  0.0048, -0.0022,  0.0027,  0.0059, -0.0004,  0.0020, -0.0035,\n",
      "         0.0009,  0.0008, -0.0168, -0.0055, -0.0050,  0.0026, -0.0048, -0.0094,\n",
      "        -0.0117,  0.0202,  0.0165,  0.0063,  0.0143, -0.0031, -0.0124, -0.0075,\n",
      "        -0.0054, -0.0045,  0.0056, -0.0063,  0.0020, -0.0078,  0.0002, -0.0075,\n",
      "         0.0235, -0.0045, -0.0015,  0.0067, -0.0121, -0.0065,  0.0117,  0.0005,\n",
      "         0.0082,  0.0055,  0.0060,  0.0022, -0.0040, -0.0043,  0.0030,  0.0053,\n",
      "         0.0074, -0.0013,  0.0064,  0.0052, -0.0060,  0.0039,  0.0133,  0.0220,\n",
      "         0.0084, -0.0190,  0.0095,  0.0040, -0.0043, -0.0012, -0.0076, -0.0004,\n",
      "         0.0005, -0.0033,  0.0145,  0.0090])\n",
      "#####################\n",
      " -> Target\n",
      "tensor(5., dtype=torch.float64)\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "From the train dataloader, an sliding window input of 10, \n",
    "the corresponding embedding of the product and the target respectively\n",
    "'''\n",
    "\n",
    "for k,v in enumerate(train_dataloader):\n",
    "    print('#####################')\n",
    "    print(' -> Sliding window sample')\n",
    "    print(v[0][0][0]) # First sliding window sample\n",
    "    print('#####################')\n",
    "    print(' -> Embbedding')\n",
    "    print(v[0][1][0]) # Embbedding from the product to which the sliding window belongs\n",
    "    print('#####################')\n",
    "    print(' -> Target')\n",
    "    print(v[1][0]) # Target (rating 'T+1' assuming that  the sliding window goes from 'T-9' to 'T')\n",
    "    print('#####################')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ee889",
   "metadata": {},
   "source": [
    "### C. The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478823f7",
   "metadata": {},
   "source": [
    "The model is structured as a LSTM cell followed by a feedforward neural network with an input layer, three hidden layers and an output layer. The The LSTM cell receives as an input the last ten ratings, while the network takes as inputs the output of the LSTM plus the corresponding product embedding.\n",
    "\n",
    "Some relevant parameters of the LSTM cell: (More detailed information can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html))\n",
    "- __Input_dim__: The number of expected features in the input x \n",
    "- __Hidden_dim__: The number of features in the hidden state h \n",
    "- __Layer_dim__: Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f11c45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim):\n",
    "        super(LSTM_1, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim # n of expected features in the input\n",
    "        self.hidden_dim = hidden_dim # Hidden dimensions - n of features in hidden state\n",
    "        self.layer_dim = layer_dim  # Number of hidden layers \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim) #LSTM\n",
    "        self.OLA = nn.Linear(110,64)\n",
    "        self.OLB = nn.Linear(64,32)\n",
    "        self.OLC = nn.Linear(32,16)\n",
    "        self.OLD = nn.Linear(16,8)\n",
    "        self.OLE = nn.Linear(8,1)\n",
    "        self.double()\n",
    "        # Note: HLA = 'Hidden Layer A', OLB = 'Hidden Layer B', and so on...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state & cell states with zeros \n",
    "        # theoretically not 100% necessary - made by default for zeros\n",
    "        h0 = torch.zeros(size=(self.layer_dim, 10), device='cuda').requires_grad_()\n",
    "        c0 = torch.zeros(size=(self.layer_dim, 10), device='cuda').requires_grad_()\n",
    "       \n",
    "        out, (hn, cn) = self.lstm(x[0].double(), (h0.double(),c0.double()))\n",
    "        \n",
    "        # OLA is fed with the combination of LSTM output and the corresponding embeddings.\n",
    "        outA = F.relu(self.OLA(torch.cat((out,x[1]), dim=1))) \n",
    "        outB = F.relu(self.OLB(outA))\n",
    "        outC = F.relu(self.OLC(outB))\n",
    "        outD = F.relu(self.OLD(outC))\n",
    "        outE = self.OLE(outD)\n",
    "        \n",
    " \n",
    "        return outE, hn, cn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7446995",
   "metadata": {},
   "source": [
    "### D. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0c99b",
   "metadata": {},
   "source": [
    "Adapted from ebmd. to ratings training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb367cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for batch, (x, y) in enumerate(train_dataloader):\n",
    "    count += (len(x*1))\n",
    "    \n",
    "train_indices_A = count\n",
    "count = 0    \n",
    "\n",
    "for batch, (x, y) in enumerate(validation_dataloader):\n",
    "    count += (len(x*1))\n",
    "    \n",
    "\n",
    "val_indices_A = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5fcc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10 \n",
    "hidden_size = 10\n",
    "num_layers = 1\n",
    "learning_rate=1e-3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LSTM_1(input_dim , hidden_size , num_layers).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05e003f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_converged_B(epoch: int, stop_after_epochs: int, validation_loss: torch.Tensor, model: torch.nn.Module) -> bool:\n",
    "    converged = False\n",
    "    # (Re)-start the epoch count with the first epoch or any improvement.\n",
    "    if epoch == 0 or validation_loss < model.best_validation_loss:\n",
    "        model.best_validation_loss = validation_loss\n",
    "        model.epochs_since_last_improvement = 0\n",
    "        model.best_model = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        model.epochs_since_last_improvement += 1\n",
    "\n",
    "    # If no validation improvement over many epochs, stop training.\n",
    "    if model.epochs_since_last_improvement > stop_after_epochs - 1:\n",
    "        model.load_state_dict(model.best_model)\n",
    "        converged = True\n",
    "    return converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51b7d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def fit(\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        train_indices: torch.Tensor,\n",
    "        val_indices: torch.Tensor,\n",
    "        num_epochs: int = 100,\n",
    "        print_per_num_epochs: int = 5,\n",
    "        convergence_num_epochs: int = 15,\n",
    "    ) -> None:\n",
    "# Run the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"epoch {epoch}\")\n",
    "        # Set the model in training mode for gradient evaluation\n",
    "        model.train()\n",
    "        # Set current loss value\n",
    "        train_loss = 0.0\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for batch, (inputs, targets) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                inputs[0] = inputs[0].cuda() \n",
    "                inputs[1] = inputs[1].cuda()\n",
    "                targets = targets.cuda()\n",
    "            # Send inputs and targets to cuda\n",
    "            #inputs.to(device)\n",
    "            #targets.to(device)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Perform forward pass\n",
    "            out, hn, cn = model(inputs)\n",
    "            out = out.reshape(len(inputs[1]))[0] # ADDED + añadiste el 0 entre corchetes para intentart arreglar el tema del broadcasting\n",
    "            # Compute loss\n",
    "            loss = criterion(out, targets).mean()\n",
    "            hn = hn.detach() # ADDED\n",
    "            cn = cn.detach() # ADDED\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            # Add to the total of the training loss\n",
    "            train_loss += loss.item() * len(inputs)\n",
    "        # Once all training batches have been run, get the mean training loss\n",
    "        #train_loss /= len(train_indices)\n",
    "        train_loss /= train_indices\n",
    "        # Set the model in evaluation mode so that gradients are not evaluated\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs[0] = inputs[0].cuda() \n",
    "                inputs[1] = inputs[1].cuda()\n",
    "                targets = targets.cuda()\n",
    "                preds, hn, cn = model(inputs)\n",
    "                loss = criterion(preds, targets).mean()\n",
    "                val_loss += loss.item() * len(inputs)\n",
    "            #val_loss /= len(val_indices)\n",
    "            val_loss /= val_indices\n",
    "        if epoch % print_per_num_epochs == 0: \n",
    "            print(f\"Train Loss after epoch: {epoch}: {train_loss}\") \n",
    "            print(f\"Validation loss after epoch: {epoch}: {val_loss}\") \n",
    "        if nn_converged_B(epoch, convergence_num_epochs, val_loss, model): \n",
    "            print(f\"Stopping after epoch {epoch} as validation loss was not improving further\") \n",
    "            break \n",
    "    \n",
    "    # Process is complete.\n",
    "    print(\"Training process has finished.\")\n",
    "    # print(f\"Best loss: {self.model.best_validation_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad127700",
   "metadata": {},
   "source": [
    "Training execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21a726d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after epoch: 0: 13.711423828734315\n",
      "Validation loss after epoch: 0: 1.2744138994517002\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "Train Loss after epoch: 5: 1.1675084397476425\n",
      "Validation loss after epoch: 5: 1.1922985872795433\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "Train Loss after epoch: 10: 1.1505751694729636\n",
      "Validation loss after epoch: 10: 1.1836869648710469\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "Train Loss after epoch: 15: 1.1441559405759067\n",
      "Validation loss after epoch: 15: 1.1810579017178864\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "Train Loss after epoch: 20: 1.1412802478139523\n",
      "Validation loss after epoch: 20: 1.1770800925413476\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "Train Loss after epoch: 25: 1.1388987870042717\n",
      "Validation loss after epoch: 25: 1.1784700654328737\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "Train Loss after epoch: 30: 1.137744483533712\n",
      "Validation loss after epoch: 30: 1.1762840250438515\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "Train Loss after epoch: 35: 1.137049477639526\n",
      "Validation loss after epoch: 35: 1.1783054097947523\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n",
      "epoch 40\n",
      "Train Loss after epoch: 40: 1.1362138514003595\n",
      "Validation loss after epoch: 40: 1.1767460086589776\n",
      "epoch 41\n",
      "epoch 42\n",
      "epoch 43\n",
      "epoch 44\n",
      "epoch 45\n",
      "Train Loss after epoch: 45: 1.1350488457990926\n",
      "Validation loss after epoch: 45: 1.176667503916713\n",
      "epoch 46\n",
      "epoch 47\n",
      "Stopping after epoch 47 as validation loss was not improving further\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "fit(train_loader = train_dataloader, val_loader = validation_dataloader, train_indices = train_indices_A, val_indices = val_indices_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fa1fd",
   "metadata": {},
   "source": [
    "### E. Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb148664",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'trained_lstm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf83c39",
   "metadata": {},
   "source": [
    "### F. Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53caf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = torch.load('trained_lstm.pkl')\n",
    "model = model_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45820e82",
   "metadata": {},
   "source": [
    "### G. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38130ae4",
   "metadata": {},
   "source": [
    "#### G.1 - New (self-updating) design for prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecb200f",
   "metadata": {},
   "source": [
    "In short, the predictions are produced as follows for each product:\n",
    "\n",
    "1. The first 10 \"real\" reviews of the test set are taken (plus the embedding) and the model generates a first prediction based on this.\n",
    "2. The first predicted rating is stored, and then for the second prediction the sliding window will take the first 9 \"real\" ratings of the test set plus the first predicted value (instead of the 11th \"real) \n",
    "3. The second predicted value is stored, and then for the third prediction the sliding window will take the first 8 \"real\" ratings and the two first predicted values.\n",
    "4. The process keeps going sequentially as described, by the time of making the 11th prediction, the model will be receiving as inputs only previously predicted-by-itself values.\n",
    "5. Predictions will be generated until the number of predicted ratings is equal to the total number of ratings in the test set minus 10 (size of the initial sliding window)\n",
    "6. The MSE between the target \"real\" ratings tensor and the predicted ratings tensor is computed an returned in a dictionary alongside the tensors.\n",
    "\n",
    "(Note that the function below can generate prediction for multiple products (chosen at random) with a single call by modifying the parameter `k_prods`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "84387d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions(k_prods=1, verbose=1):\n",
    "    '''\n",
    "    Generates predictions for a number (determined by the parameter k_prods) of products chosen at \n",
    "    random from the available products and returns a dictionary containing each product Id plus the \n",
    "    prediction, the actual ratings and the MSE between the two. \n",
    "    \n",
    "    Information about the prediction process and results can be printed. Such information is governed \n",
    "    by the parameter verbose. A value of 0 for the parameter will emit 0 information while 3 will emit \n",
    "    the maximum level of information. (1 and 2 are the other two accepted values)\n",
    "    '''\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    #Choosing 'k_prods' random products to test and looping over them\n",
    "    for i in random.choices(list(tens_test.keys()), k=k_prods):\n",
    "        if verbose>1:\n",
    "            print('##########################################')\n",
    "            print('Generating rating prediction for product_'+str(i))\n",
    "        prod_k = i\n",
    "        \n",
    "        num_test_ratings = len(tens_test.get(i)) #Number of ratings in product 'k' test set\n",
    "        num_test_ratings_to_predict = num_test_ratings - sliding_window_size #Number of ratings in product 'k' test set minus sliding window length\n",
    "        if verbose>1:\n",
    "            print(f\"predictions to generate: {num_test_ratings_to_predict}\")\n",
    "        embedding_i = tens_test.get(i)[:1, 1:] #Embedding of product i\n",
    "        predicted_ratings = tens_test.get(i)[0:sliding_window_size].reshape((sliding_window_size)*101) #Initialize tensor to store predictions\n",
    "        \n",
    "        #Getting dataloader of size window_size +1 for each product i (t_0)\n",
    "        start_window_target = tens_test.get(i)[:sliding_window_size+1]\n",
    "        start_wt_set = SlidingDataset(torch.from_numpy(np.array(start_window_target)), sliding_window_size)\n",
    "        start_wt_loader = DataLoader(start_wt_set)\n",
    "        \n",
    "        #feeding the dataloader to the model & predicting (prediction 0)\n",
    "        for batch, (inputs, targets) in enumerate(start_wt_loader):\n",
    "            if verbose>2:\n",
    "                print(f\"initial input (t=0): {inputs[0]}\")\n",
    "            inputs[0] = inputs[0].cuda() \n",
    "            inputs[1] = inputs[1].cuda()\n",
    "            targets = targets.cuda()\n",
    "            model.eval()\n",
    "            prediction = model(inputs)\n",
    "            prediction = round(float(prediction[0].reshape(len(inputs[1]))[0]), 0)\n",
    "            if verbose>2:\n",
    "                print(f'initial prediction: {prediction}')\n",
    "            \n",
    "            prediction_plus_embed = torch.concat((torch.tensor(prediction).reshape(1), embedding_i[0]))\n",
    "            predicted_ratings = torch.concat((predicted_ratings, prediction_plus_embed))\n",
    "            if verbose>2:\n",
    "                print(f\"current size of predicted_ratings: {int(predicted_ratings.shape[0])/101}\")\n",
    "            \n",
    "        for n in range(num_test_ratings_to_predict-1):\n",
    "            \n",
    "            old_ratings = predicted_ratings[(n+1)*101:(n+sliding_window_size+1)*101] \n",
    "            #old_plus_pred = torch.concat((old_ratings, prediction_plus_embed))\n",
    "            '''\n",
    "            A target must be included even if it will not be considered during prediction\n",
    "            given that the input must be a tensor of shape (sliding window+1, 101)\n",
    "            '''\n",
    "            old_pred_target = torch.concat((old_ratings, tens_test.get(i)[sliding_window_size+1]))\n",
    "            loadable_package = old_pred_target.reshape(sliding_window_size+1,101) \n",
    "            \n",
    "            \n",
    "            wt_set = SlidingDataset(loadable_package, sliding_window_size)\n",
    "            wt_loader = DataLoader(wt_set)\n",
    "            \n",
    "            \n",
    "            for batch, (inputs, targets) in enumerate(wt_loader):\n",
    "                if verbose>2:\n",
    "                    print(f\"input at time {n}: {inputs[0]}\")\n",
    "                inputs[0] = inputs[0].cuda() \n",
    "                inputs[1] = inputs[1].cuda()\n",
    "                targets = targets.cuda()\n",
    "                model.eval()\n",
    "                prediction = model(inputs)\n",
    "                prediction = round(float(prediction[0].reshape(len(inputs[1]))[0]), 0)\n",
    "                if verbose>2:\n",
    "                    print(f'prediction at time n: {prediction}')\n",
    "            \n",
    "            prediction_plus_embed = torch.concat((torch.tensor(prediction).reshape(1), embedding_i[0]))\n",
    "            predicted_ratings = torch.concat((predicted_ratings, prediction_plus_embed))\n",
    "            if verbose>2:\n",
    "                print(f\"current shape of predicted_ratings: {int(predicted_ratings.shape[0])/101}\")\n",
    "            \n",
    "        predicted_ratings_final = predicted_ratings.reshape(num_test_ratings, 101)[10:,0]\n",
    "        target_ratings = tens_test.get(i)[10:,0]\n",
    "        results_to_dict = torch.concat((predicted_ratings_final, target_ratings)).reshape(2,num_test_ratings_to_predict)\n",
    "        loss = nn.MSELoss()\n",
    "            \n",
    "        results[i] = (results_to_dict, loss(predicted_ratings_final, target_ratings))\n",
    "        if verbose>1:\n",
    "            print(\"Done\")\n",
    "        \n",
    "    if verbose>0:\n",
    "        print(\"###############\")\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"###############\")\n",
    "        \n",
    "    for k in results.keys():\n",
    "        if verbose>0:\n",
    "            print('##########################################')\n",
    "            print(f\"Product_{k}\")\n",
    "            print(f\"generated predictions: {len(results.get(k)[0][1])}\")\n",
    "            print(f\"MSE for product {i}: {results.get(k)[1]}\")\n",
    "        if verbose>1:\n",
    "            print(f\"Generated predictions for product {i}: \\n {results.get(k)[0][0]}\")\n",
    "            print(f\"Actual ratings of product {i}: \\n {results.get(k)[0][1]}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c4b8cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "SUMMARY\n",
      "###############\n",
      "##########################################\n",
      "Product_1396407\n",
      "generated predictions: 25\n",
      "MSE for product 1396407: 1.2000000476837158\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Number of products to produce predictions on (at random)\n",
    "k_prods = 1\n",
    "# Verbosity of predictor (must be int in interval [0,3])\n",
    "verbose = 1\n",
    "\n",
    "generate_predictions(k_prods=k_prods, verbose=verbose)\n",
    "print(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320b543",
   "metadata": {},
   "source": [
    "### H. OLD-DISCARDED: Loading/Saving model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e4272497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_saved = model.best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5b307bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters are stores as an OrderedDict\n",
    "#type(best_params_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c528fad",
   "metadata": {},
   "source": [
    "Saving best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "49afdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('stored_model_0.pickle', 'wb') as handle:\n",
    "#    pickle.dump(best_params_saved, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe823b",
   "metadata": {},
   "source": [
    "Loading params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "133bbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('stored_model_0.pickle', 'rb') as handle:\n",
    "#    best_params_loaded = pickle.load(handle)\n",
    "    \n",
    "#model.load_state_dict(best_params_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bba96a",
   "metadata": {},
   "source": [
    "### I. OLD-DISCARDED: design for serial prediction (not self-updating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c6ad2f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_9031537\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_368/1620461863.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_prods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreal_ratings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction_round\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prediction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Real data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#results = {}\n",
    "##Choosing 'k_prods' random products to test and looping over them\n",
    "#for i in random.choices(list(tens_test.keys()), k=k_prods):\n",
    "#    print('product_'+str(i))\n",
    "#    \n",
    "#    #Transforming each product's test set into a dataloader\n",
    "#    c = SlidingDataset(torch.from_numpy(np.array(tens_test.get(i))), 10)\n",
    "#    test_product_i = DataLoader(c,len(tens_test.get(i)))\n",
    "#\n",
    "#    #feeding the dataloader to the model & predicting\n",
    "#    for batch, (inputs, targets) in enumerate(test_product_i):\n",
    "#        inputs[0] = inputs[0].cuda() \n",
    "#        inputs[1] = inputs[1].cuda()\n",
    "#        targets = targets.cuda()\n",
    "#        model.eval()\n",
    "#        prediction = model(inputs)\n",
    "#    \n",
    "#    # Collecting prediction vs targets, transforming them into histogram format\n",
    "#    prediction_round=[round(float(e), 0) for e in prediction[0]]\n",
    "#    real_ratings = [int(a[0]) for a in tens_test.get(i)]\n",
    "#    \n",
    "#    \n",
    "#    real_ratings_hist = []\n",
    "#    predic_ratings_hist = []\n",
    "#    \n",
    "#    for o in range(1,6):\n",
    "#        real_ratings_hist.append(real_ratings[10:].count(o))\n",
    "#        predic_ratings_hist.append(prediction_round.count(o))\n",
    "#        \n",
    "#    # Adding a tuple with predictions and targets to the results dictionary\n",
    "#    results['product_' + str(i)] = (real_ratings_hist, predic_ratings_hist)\n",
    "#    \n",
    "#    if len(random.choices(list(tens_test.keys()), k=k_prods)) == 1:\n",
    "#        g = sns.histplot(data=[real_ratings,prediction_round], bins=5, discrete=True, legend=False).set(title=('product_'+str(i)))\n",
    "#        plt.legend(loc='upper left', labels=['Prediction', 'Real data'])\n",
    "#        plt.show(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30edeb",
   "metadata": {},
   "source": [
    "### J. OLD-DISCARDED: LSTM model for classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class LSTM_0(nn.Module):\n",
    "#    def __init__(self, input_dim, hidden_dim, layer_dim):\n",
    "#        super(LSTM_0, self).__init__()\n",
    "#        \n",
    "#        self.input_dim = input_dim # n of expected features in the input\n",
    "#        self.hidden_dim = hidden_dim # Hidden dimensions - n of features in hidden state\n",
    "#        self.layer_dim = layer_dim  # Number of hidden layers \n",
    "#        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim) #LSTM\n",
    "#        self.output_layer = nn.Linear(10,1)\n",
    "#        self.double()\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        # Initialize hidden state & cell states with zeros \n",
    "#        # theoretically not 100% necessary - made by default for zeros\n",
    "#        h0 = torch.zeros(self.layer_dim, 10).requires_grad_()\n",
    "#        c0 = torch.zeros(self.layer_dim, 10).requires_grad_()\n",
    "#\n",
    "#        out, (hn, cn) = self.lstm(x.double(), (h0.double(),c0.double()))\n",
    "#        out2 = self.output_layer(out)\n",
    "#        #out, (hn, cn) = self.lstm(x)\n",
    "#        #final_out = self.readout(out[-1])\n",
    "# \n",
    "#        return out, hn, cn #, final_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
